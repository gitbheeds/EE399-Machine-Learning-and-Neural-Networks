{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAUNAK BHIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = loadmat('yalefaces.mat')\n",
    "X = results['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a\n",
    "#grab first 10 faces\n",
    "#Create labels for all photos\n",
    "X_10 = X[:, :640]\n",
    "y = np.repeat(range(10), 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B: pick 14 scenes from each face to use as test data, and 50 scenes to use as training data\n",
    "#Doing this for every face.\n",
    "np.random.seed(70)\n",
    "lighting_scenes = 64\n",
    "faces_to_extract = 10\n",
    "test_scenes = 14\n",
    "train_scenes = 50\n",
    "\n",
    "# Shuffle lighting scenes within each face\n",
    "for i in range(faces_to_extract):\n",
    "    face_start = i * lighting_scenes\n",
    "    face_end = (i + 1) * lighting_scenes\n",
    "    np.random.shuffle(X_10[:, face_start:face_end])\n",
    "\n",
    "# Initialize training and test matrices\n",
    "train_data = np.zeros((1024, faces_to_extract * train_scenes))\n",
    "test_data = np.zeros((1024, faces_to_extract * test_scenes))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "for i in range(faces_to_extract):\n",
    "    face_start = i * lighting_scenes\n",
    "    train_start = i * train_scenes\n",
    "    test_start = i * test_scenes\n",
    "\n",
    "    train_data[:, train_start:train_start + train_scenes] = X_10[:, face_start:face_start + train_scenes]\n",
    "    test_data[:, test_start:test_start + test_scenes] = X_10[:, face_start + train_scenes:face_start + lighting_scenes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 64.29%\n"
     ]
    }
   ],
   "source": [
    "# Part C:\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Perform PCA on the training data\n",
    "n_modes = 20\n",
    "pca = PCA(n_components=n_modes)\n",
    "pca_train_data = pca.fit_transform(train_data.T)\n",
    "\n",
    "# Create training labels\n",
    "train_labels = np.repeat(np.arange(faces_to_extract), train_scenes)\n",
    "\n",
    "# Perform LDA on the reduced training data\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(pca_train_data, train_labels)\n",
    "\n",
    "# Transform the test data using the same PCA model\n",
    "pca_test_data = pca.transform(test_data.T)\n",
    "\n",
    "# Create test labels\n",
    "test_labels = np.repeat(np.arange(faces_to_extract), test_scenes)\n",
    "\n",
    "# Make predictions on the reduced test data\n",
    "predictions = lda.predict(pca_test_data)\n",
    "\n",
    "# Calculate classification accuracy\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Classification accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification accuracy: 82.86%\n",
      "Decision Tree Classification accuracy: 87.14%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Part D: \n",
    "# Classifying faces using an SVM and a decision tree\n",
    "# SVM classifier\n",
    "\n",
    "# Transform the test data using the same PCA model\n",
    "pca_test_data = pca.transform(test_data.T)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(pca_train_data, train_labels)\n",
    "y_pred_svm = svm.predict(pca_test_data)\n",
    "accuracy_svm = accuracy_score(test_labels, y_pred_svm)\n",
    "print(f\"SVM Classification accuracy: {accuracy_svm * 100:.2f}%\")\n",
    "\n",
    "# decision tree classifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(pca_train_data, train_labels)\n",
    "y_pred_dtc = dtc.predict(pca_test_data)\n",
    "accuracy_dtc = accuracy_score(test_labels, y_pred_dtc)\n",
    "print(f\"Decision Tree Classification accuracy: {accuracy_dtc * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM and decision tree were both very close, and able to classify the different faces the best, followed by the the LDA. Random seeds for the SVM and decision tree do affect the accuracy. The main reason for this is that we are randomly picking lighting scenes for each face and putting them all into the same datasets, so it is harder to linearly separate the data. \n",
    "\n",
    "SVM is a powerful classification algorithm that works by finding the best hyperplane that separates the data points of different classes in the feature space. SVM can handle high-dimensional data well and is robust against overfitting. In this case the SVM model was able to find a good separation boundary between the different faces in the dataset. The SVM is also able to handle nonlinearly seperable data.\n",
    "\n",
    "Decision trees, on the other hand, are a simple and interpretable classification method that works by recursively splitting the data based on the feature that provides the most information gain. While decision trees can handle non-linearly separable data and can be easily visualized, they are prone to overfitting and can become very complex. In this case, the decision tree model achieved a good result.\n",
    "\n",
    " LDA works well when the classes are well-separated and the data is normally distributed. However, it may perform poorly when the data is highly skewed or has overlapping classes. In this case, the LDA model achieved an accuracy of 62.14%, which is significantly lower than the SVM and decision tree models. This could be due to the fact that the yalefaces dataset has high within-class variance and low between-class variance, making it difficult for the LDA model to find a good separation boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
